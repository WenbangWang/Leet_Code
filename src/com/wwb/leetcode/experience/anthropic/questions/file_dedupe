给定一个文件夹目录，找出其中所有的重复文件。

整个面试过程都在一个共享的 Google Colab 上进行，我们用平台上的文件来测试代码的正确性。在实现了一个基本的解决方案（即对每个文件的完整内容进行哈希）后，面试官开始了一系列追问。他首先询问我实现的时间复杂度，我解释说这与所有文件大小的总和成正比。

为了改进效率，我提出了一种多阶段的方法，先使用“廉价”的哈希函数。具体来说，首先从元数据检查文件大小，然后对大小相同的文件哈希其前1024个字节，最后才在万不得已时进行完整的哈希。面试官随后问这种新方法在最坏情况下的时间复杂度，我回答说，如果所有文件的大小和初始内容都完全相同，那么时间复杂度仍然取决于文件的总大小。
这引出了关于哈希函数的更深层次讨论。我当时苦苦思索，试图找到一个既廉价又哈希碰撞率低的哈希函数。面试后我才意识到，没有哪个哈希函数是完美的，你总能找到反例。接着，话题转向了系统设计，面试官问我如何判断一个程序是CPU密集型还是I/O密集型。我解释说会用性能分析结果来判断：如果CPU大部分时间处于空闲状态，程序就是I/O密集型；如果CPU持续繁忙，则是CPU密集型。
之后，面试官提出了一个更复杂的挑战：设计一个持续监控重复文件的系统。这个问题有些模糊，所以我先与他确认了需求：系统需要在添加重复文件时通知文件所有者，并且要能处理文件的删除。我概述了一个解决方案，即使用数据库来维护两个映射：一个是从文件哈希到文件的映射（用于重复检测和通知），另一个是从文件到其哈希的映射（用于处理删除）。我还提到了对于海量文件，可以使用类似 MapReduce 的机制来扩展，并解释了 Map 和 Reduce 操作分别会做什么，以高效地处理数据。

文件大小 → 弱哈希 → 完整哈希
